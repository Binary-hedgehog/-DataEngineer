# Подготовка к собеседованию для Spark Data Engineer
## Оглавление
+ [Spark](#spark)
  + [Spark Application](#Spark-Application)
  + [Основные компоненты Spark](#Основные-компоненты-Spark)
  + [Структуры данных в Spark](#Структуры-данных-в-Spark)
  + [Оптимизация в Spark](#Оптимизация-в-Spark)
  + [Joins](#Joins)
  + [Ect](#Ect)
      + [Отличия Spark и PySpark](#Отличия-Spark-и-PySpark)
      + [MapReduce как аналог Spark](#MapReduce-как-аналог-Spark)
      + [Файловые системы, поддерживаемые Spark](#Файловые-системы,-поддерживаемые-Spark)
      + [Udf](#Udf)
      + [Кэширование](#Кэширование)
+ [Оркестраторы](#оркестраторы)
+ [Прочие полезности Big Data](#прочие-полезности)
  + [Hadoop](#hadoop)
  + [Kafka](#Kafka)
  + [Flink](#Flink)
  + [Хранение данных](#Хранение-данных)
  + [Форматы данных в BigData](#Форматы-данных-в-BigData)
  + [ETL инструменты](#ETL-инструменты)
+ [Источники](#Источники)
---
## Другие файлы
* [Python](https://github.com/Binary-hedgehog/-DataEngineer/blob/main/Python.md)
* [Scala](https://github.com/Binary-hedgehog/-DataEngineer/blob/main/Scala.md)
* [SQL](https://github.com/Binary-hedgehog/-DataEngineer/blob/main/SQL.md)
* [Общая информация](https://github.com/Binary-hedgehog/-DataEngineer/blob/main/Common.md)
* [Вопросы работодателю](https://github.com/Binary-hedgehog/-DataEngineer/blob/main/Questions%20for%20employer.md)
---
## Spark
[Go Back](#оглавление)
* Apache Spark — это фреймворк для обработки и анализа больших объёмов информации
* Чем хорош Spark?
    * Поддерживает такие языки программирования как Scala, Java, Python, R и SQL
    * Обработка данных в оперативной памяти позволяет значительно ускорить выполнение программы относительно MapReduce
    * Использование отложенных вычислений
    * Хорошо поддерживает параллелизм
    * Если на кластере есть Hadoop, то он из коробки поддерживает Spark, но для работы Spark Hadoop не обязателен
* Структура Spark
![Структура Spark](https://habrastorage.org/getpro/habr/upload_files/289/a96/15b/289a9615bb238133ebb9a35940b1fece.png)
### Spark Application
[Go Back](#оглавление)
#### Как работает Spark
1. Создается Spark сессиия
    * Точка входа к Spark - это _SparkContext_, он настраивает внутренние сервисы, необходимые, чтобы установить связь со средой выполнения Spark.
    * _SparkConf_ хранит параметры конфигурации приложения
2. Код, который нужно исполнить Spark передается на **Driver**, где
    * Строится план (логический, затем физический) запроса(ов) при помощи **Catalyst**
    * Распланированные запросы передаются на запуск в **Executors**
3. Выполнение **Jobs** на **Executors**
    * **Job** = Action
        * То есть один **Job** это одно действие
    * **Job** разделяется на **Stages**
        * **Stages** - это трансформации в рамках одного действия
    * **Stages** делятся на **tasks**
        * Перемешай, соедени, переложи и тд. 
    * **Executors** передают информацию о процессе вычислений на **Driver**
        * **Driver** же в свою очередь постоянно занимается выделением ресурсов общаясь с **Cluster Manager**
    * Каждый Executor имеет определенное количество ядер, которые отвечают за параллельные вычисления
4. Если **Driver** завершает работу, приложение закончено
#### Дополнительно
* На Driver и Executors по умолчанию выделяется 1гб ОП
* Cluster Manager
    * Занимается управлением реальными машинами кластера и контролирует выделение ресурсов (он же ресурсный менеджер)
    * Виды: **Standalone cluster**, **Apache Mesos** и **YARN**
* Планировщик Spark
    * Полностью потокобезопасен и поддерживает этот вариант использования, чтобы включить приложения, которые обслуживают несколько запросов (например, запросы для нескольких пользователей)
    * По умолчанию запускает задания в режиме FIFO (first in, first out)
    * Каждое задание делится на этапы «stages» (например, фазы map и reduce), и первый job получает приоритет на всех доступных ресурсах, пока на его этапах есть задачи для запуска, затем второй job получает приоритет и т. д.
    * Если job в начале очереди не нужно использовать весь кластер, более поздние job могут начать выполняться сразу, но если job в начале очереди большие, то более поздние job могут значительно задерживаться
![a](https://habrastorage.org/r/w1560/getpro/habr/upload_files/f35/582/063/f355820631bc7e8205852bdbeb24b3f1.png)
### Основные компоненты Spark
[Go Back](#оглавление)
#### Spark Core
* Включает функции управления памятью, для этого имеется специальное API
* Восстановление в случае аварий
* Планирование и управление задачами на кластере
* Взаимодействие с хранилищем, распределение данных
#### Spark SQL 
* Механизм запросов SQL
* Использует диалект SQL: Hive Query Language (HQL)
* Умеет работать с множеством различных источников данных, таких как таблицы Hive, Parquet и JSON
* Позволяет смешивать в одном приложении запросы SQL с программными конструкциями на Python, Java и Scala, поддерживаемыми абстракцией RDD, и таким способом комбинировать SQL со сложной аналитикой
#### Spark Streaming
* Модуль Streaming содержит методы для обработки данных в реальном времени. Он отбирает отдельные блоки из общего потока данных, переводит их в форму мультимножества и передаёт в другие модули Spark
* Spark Streaming имеет API для управления потоками данных, соответствующий модели RDD, поддерживаемой компонентом Spark Core
#### MLlib 
* Эта библиотека для машинного обучения включает в себя различные алгоритмы классификации, регрессии, кластеризации данных и так далее
#### GraphX 
* Библиотека содержит объекты и методы, которые упрощают обход и анализ графовых структур
* В GraphX есть готовый набор алгоритмов для решения типовых задач графовой аналитики: ранжирования страниц, SVD++, подсчёта треугольников, обнаружения сообществ и так далее
* GraphX дополняет Spark RDD API возможностью создания ориентированных графов с произвольными свойствами, присваиваемыми каждой вершине или ребру
* Также GraphX поддерживает разнообразные операции управления графами (такие как subgraph и rnapVertices) и библиотеку обобщенных алгоритмов работы с графами
### Структуры данных в Spark
[Go Back](#оглавление)
#### RDD - Resilient Distributed Dataset
* Это определенный набор объектов, разбитых на блоки partitions
    * Partitions могут храниться на разных узлах кластера
* Блоком, или партицией, можно считать цельную логическую неизменяемую часть данных, создать которую можно в том числе посредством преобразования (**transformations**) уже существующих блоков
* RDD может быть представлен как в виде структурированных наборов данных, так и неструктурированных
* RDD отказоустойчивы и могут быть восстановлены в случае сбоя
* Data source API позволяет RDD формироваться из любых источников включая текстовые файлы, при этом необязательно даже структурированных
* RDD сериализуется каждый раз, когда Spark требуется распределить данные внутри кластера или записать информацию на диск
#### DataFrame
* Это набор типизированных записей, разбитых на блоки.
    * Иными словами — таблица, состоящая из строк и столбцов
* Блоки могут обрабатываться на разных нодах кластера
* DataFrame может быть представлен только в виде структурированных или полуструктурированных данных
* Данные представлены именованным набором столбцов, напоминая таблицы в реляционных БД
* Может сериализовать данные в хранилище вне кучи (т.е., в памяти) в двоичном формате, а затем выполнить много преобразований непосредственно в этой памяти без кучи
    * Это возможно из-за того, что в датафреймах Spark понимает схему данных
    * Благодаря инструменту _Tungsten_ не требуется использовать сериализацию Java для кодирования данных, т.к. этот механизм явно управляет памятью и динамически генерирует байт-код для оценки выражений
    * Благодаря динамической генерации байт-кода с сериализованными данными можно выполнить много операций, не требуя десериализации для небольших вычислений
* Можно создать из RDD
    * Обратная же конвертация создаст **новый** RDD объект 
* Data source API позволяет обрабатывать разные форматы файлов (AVRO, CSV, JSON, а также из систем хранения HDFS, HIVE таблиц, MySQL)
#### DataSet
* DataSet это коллекциия с элементами типа Row (и по факту это расширение API DataFrame)
* Обеспечивает функциональность объектно-ориентированного RDD-API (строгая типизация, лямбда-функции), производительность оптимизатора запросов _Catalyst_ и механизм хранения вне кучи
* DataSet эффективно обрабатывает структурированные и неструктурированные данные, представляя их в виде строки JVM-объектов или коллекции
* Обрабатывает преобразование между объектами JVM в табличное представление c использованием вышеупомянутого безопасного двоичного формата _Tungsten_
    * Это позволяет выполнить операцию с сериализованными данными, эффективно используя память, т.к. предоставляется доступ к индивидуальному атрибуту по требованию без десериализации всего объекта
* Функционал Spark позволяет преобразовывать как RDD так и DataFrame в сам DataSet
* Dataset API также поддерживает различные форматы данных
### Сравнение
* RDD - это самый простой низкоуровневый API, обеспечивающий больший контроль над данными, но с низкоуровневой оптимизацией
* DataFrames предоставляют высокоуровневый API, оптимизированный по производительности и более простой в работе со структурированными данными
* Datasets похожи на фреймы данных по производительности, но с более строгой типизацией и генерацией кода, что делает их хорошим выбором для высокопроизводительной пакетной и потоковой обработки с строгой типизацией

| Context | RDD | DataFrame | Dataset | 
| ------- | --- | --------- | ------- |
| Совместимость | Может быть преобразован в DataFrames `toDF()` | Может быть преобразован в RDD и Datasets с помощью методов `rdd()` и `as[]` соответственно | Может быть легко преобразован в DataFrames с помощью `toDF()` метода и в RDD с помощью `rdd()` метода |
| Безопасность типов | Не типобезопасен | DataFrames не типобезопасен. Когда мы пытаемся получить доступ к столбцу, которого нет в таблице, Dataframe API не поддерживает ошибку времени компиляции. Он обнаруживает ошибки атрибутов только во время выполнения | Datasets типобезопасны, они обеспечивают проверку типов во время компиляции, что помогает выявлять ошибки на ранних стадиях процесса разработки |
| Производительность | Низкоуровневый API с большим контролем над данными, но более низкоуровневой оптимизацией по сравнению с DataFrames и Datasets | Оптимизирован для повышения производительности благодаря высокоуровневому API, оптимизатору Catalyst и генерации кода | Datasets работают быстрее, чем DataFramesпотому что они используют генерацию байт-кода JVM для выполнения операций с данными. Это означает, что Datasets могут использовать преимущества возможностей оптимизации JVM, таких как JIT-компиляция точно в срок, для ускорения обработки |
| Управление памятью | Обеспечивают полный контроль над управлением памятью, поскольку они могут быть кэшированы в памяти или на диске по выбору пользователя | Более оптимизированное управление памятью благодаря Spark SQL optimizer, который помогает сократить использование памяти | Эта двоичная структура часто занимает гораздо меньше памяти, а также оптимизирована для повышения эффективности обработки данных |
| Сериализация | Всякий раз, когда Spark необходимо распределить данные внутри кластера или записать их на диск, он использует сериализацию Java. Накладные расходы на сериализацию отдельных объектов Java и Scala являются дорогостоящими и требуют отправки как данных, так и структуры между узлами | DataFrames используют универсальный кодировщик, который может обрабатывать объекты любого типа | Datasets сериализуются с использованием специализированных кодировщиков, оптимизированных для повышения производительности |
| API | Предоставляет низкоуровневый API, который требует больше кода для выполнения преобразований и действий с данными | Предоставляет высокоуровневый API, упрощающий выполнение преобразований и действий с данными | Datasets предоставляют более богатый набор API. Datasets поддерживают как функциональную, так и объектно-ориентированную парадигмы программирования и предоставляют более выразительный API для работы с данными |
| Применение схемы | Не имеют явной схемы и часто используются для неструктурированных данных | DataFrames применяют схему во время выполнения. Имеют явную схему, описывающую данные и их типы | Datasets применяют схему во время компиляции. Имеют явную схему, которая описывает данные и их типы и строго типизирована |
| Поддержка ЯП | RDD API доступны на языках Java, Scala, Python и R | Доступно на Java, Python, Scala и R | Доступно только в Scala и Java |
| Оптимизация | В RDD отсутствует встроенный механизм оптимизации | Для оптимизации используется catalyst optimizer | Включает концепцию оптимизатора Dataframe Catalyst optimizer для оптимизации планов запросов |
| Типы данных | Поддерживает основные типы данных |  DataFrames поддерживает большинство доступных типов данных | Datasets поддерживают все те же типы данных, что и  DataFrames, но они также поддерживают пользовательские типы. Datasets более гибки, когда дело доходит до работы со сложными типами данных |
| Примеры использования | Подходит для низкоуровневой обработки данных и пакетных заданий, требующих детального контроля над данными |  Подходит для структурированной и полуструктурированной обработки данных с более высоким уровнем абстракции | Подходит для высокопроизводительной пакетной и потоковой обработки с использованием строгой типизации и функционального программирования |
### Оптимизация в Spark
[Go Back](#оглавление)
#### Lazy evaluations
* Операции в Spark делятся на два типа
  * **Преобразования** (Transformations): это операции, которые при применении к RDD возвращают ссылку на новый RDD, созданный посредством преобразования. Некоторые из наиболее часто используемых преобразований — это _filter_ и _map_
    * **Узкие преобразования** (Narrow transformations) — это когда нет перемещения данных между разделами. Преобразование применяется к данным каждого раздела RDD, и создается новый RDD с тем же числом разделов. Например, _filter_ — это узкое преобразование, потому что фильтрация применяется к данным каждого раздела, а полученные данные представляют раздел во вновь созданном RDD
    * **Широкие преобразования** (Wide transformations) требуют перемещения данных между разделами или так называемого перемешивания. Данные перемещаются с целью создания нового RDD. Например, _sortBy_ сортирует данные на основе определенного столбца и возвращает новый RDD
  * **Действия** (Actions): При применении к RDD возвращается значение. Например, _count_ возвращает количество элементов в RDD драйверной программе, или функция _collect_ возвращает сами данные
* Ленивые (отложенные) вычисления позволяют отодвигать вычисления **всех** преобразований до момента применения действия
#### Catalyst Optimizer 
* **Catalyst** — оптимизатор запросов, который работает на **Driver**
    * Поддерживает как SQL‑запросы, так и DataFrame API
    * Оптимизатор **Catalyst** использует расширенные функции языка программирования для построения более оптимальных запросов
* Основной тип данных в **Catalyst** — это дерево, состоящее из узловых объектов. Каждый узел имеет тип и дочерние узлы (а может их не иметь). Новые типы узлов определены в Scala как подклассы класса `TreeNode`. Эти объекты неизменяемы, и ими можно управлять с помощью функциональных преобразований
* К узлам дерева **Catalyst** применяет набор правил для их оптимизации, которая выполняется в четыре этапа, как показано на диаграмме ниже:
![b](https://spark-school.ru/wp-content/uploads/2021/02/1-1.png)
* Здесь, логический план составляет дерево, которое описывает, что нужно сделать; тогда как физический план точно описывает, как нужно делать
1. Анализ - определить тип каждого передаваемого столбца и действительно ли существуют столбцы, которые вы используете
2. Логическая оптимизация - оптимизатор затрат использует статистику и мощности машины для поиска наиболее эффективного плана выполнения вместо простого применения набора правил
3. Физический план - генерирует несколько физических планов на основе логического плана для выбора наиболее эффективного из всех предложенных
4. Генерация кода 
    * Создание байт-кода Java для запуска на каждой машине
    * **Catalyst** использует `QuasiQuotes`, специальную функцию Scala для преобразования дерева задания в абстрактное синтаксическое дерево (AST), которое затем компилирует и запускает сгенерированный код
#### Tungsten 
* **Tungsten** - еще один механизм оптимизации, который участвует в создании оптимального кода для выполнения запросов за счет повышения эффективности процессора и снижения нагрузки на память
* Именно благодаря **Tungsten** DataFrame работают в Spark быстрее чем RDD, т.к. не требуется использовать сериализацию Java для кодирования данных – механизм сам явно управляет памятью и динамически генерирует байт-код для оценки выражений
    * DataSet API обрабатывает преобразование между объектами JVM в табличное представление c использованием безопасного двоичного формата **Tungsten**, чтобы выполнить операцию с сериализованными данными, эффективно используя память, т.к. предоставляется доступ к индивидуальному атрибуту по требованию без десериализации всего объекта
* Основные задачи **Tungsten** 
    * Управление памятью и двоичная обработка: использование семантики приложений для контролируемого управления памятью и устранения излишних расходов на объектную модель JVM и сбор мусора
    * Вычисления с учетом кэша: алгоритмы и структуры данных для использования иерархии памяти
    * Генерация кода: использование генерации кода для современных компиляторов и процессоров
    * Никаких диспетчеризаций виртуальных функций: это уменьшает количество вызовов ЦП, которые могут оказать глубокое влияние на производительность при отправке миллиарды раз
    * Промежуточные данные в памяти против регистров ЦП: **Tungsten** Phase 2 помещает промежуточные данные в регистры ЦП. Это на порядок сокращает количество циклов для получения данных из регистров ЦП, а не из памяти
    * Разворачивание циклов и SIMD: оптимизирует механизм выполнения Apache Spark, чтобы использовать преимущества современных компиляторов и способности процессоров эффективно компилировать и выполнять простые циклы (в отличии от сложных графиков вызов функций)
* **Tungsten** реализует собственный формат сериализации, называемого небезопасной строкой (UnsafeRow). Он быстрее и компактнее Kryo и сериализация Java
    * Этот формат называется небезопасным, поскольку он представляет собой изменяемую внутреннюю необработанную память, позволяя явно управлять памятью без необходимости в объектной модели JVM
    * Он вводит новое представление `Sun.misc.Unsafe` для объектов
    * Объекты JVM известны своими накладными расходами на пространство и затратами на сборку мусора. Небезопасные объекты занимают меньше места и снижают нагрузку на ЦП, избегая сборки мусора вне кучи
* **Tungsten** ускоряет SQL-запросы, применяя более быстрое выполнение сортировки и хеширования для операций агрегирования, объединения и перемешивания
* Оффтоп: **Tungsten** переводится как Вольфрам – самый тугоплавкий редкоземельный металл, который активно используется в лампах накаливания и других осветительных приборах, что близко к переводному значению **Spark** (искра)
#### Spark Jobs optimization
* Spark предоставляет механизм для динамической корректировки ресурсов, которые занимает приложение, в зависимости от рабочей нагрузки. Это означает, что приложение может возвращать ресурсы кластеру, если они больше не используются и запрашивать их снова позже, когда возникнет потребность. Эта функция особенно полезна, если несколько приложений совместно используют ресурсы в кластере Spark
#### Самостоятельная оптимизация
* Использование DataFrame вместо RDD, так как последние проигрывают в плане оптимизации вычислений
* Кэшировать данные чтобы избегать дополнительного чтения и преобразования при повторяющихся Actions
* Партиционировать данные, при этом стараться избегать перекосов
* Помнить, что Join это дорого
* Фильтровать неиспользуемые данные
* Стараться не использовать UDF (особенно в PySpark) так как они не оптимизируются
### Joins
[Go Back](#оглавление)
* Справочная информация
    * Self-join в SQL – это тип операции соединения, при которой таблица объединяется сама с собой
    * Equi Join - эквивалентные джойны включают одно условие равенства, либо несколько, которые должны выполняться одновременно `(A.x == B.x)` либо `((A.x == B.x) и (A.y == B.y))`
    * Non-Equi Join - неэквивалентные джойны не подразумевают условий равенства `(A.x > B.x)`
    * **Shuffle** в Spark работает следующим образом:
       * Вычитываются все ключи
       * Определяется количество будущих партиций
       * Происходит логическое разбиение, какие ключи куда пойдут
       * Осуществляется перемешивание данных
       * Количество партиций регулируется параметром `spark.sql.shuffle.partitions` равным по умолчанию 200 
       * Все **Join** на основе **Shuffle** поддерживают **Full Outer**
* Виды Join
  * Left/Right Outer - оставляет все из левой таблицы, добавляет туда то, что нашел в правой. Несцепленные строки из левой заполняет null в колонках правой
  * Full Outer - сцепляет все что можно слева и спава, остальное в null, все как мы привыкли, в итоге получим все
  * Inner - оставляет то, что сцепилось из левой и правой таблицы
  * Cross - все на все, умножаем поля слева на поля справа (с точки зрения результирующего количества полей)
  * Semi (Left/Right) - оставляет только то из левой таблицы, что нашлось в правой
  * Anti (Left/Right) - оставляет только то из левой таблицы, что **НЕ** нашлось в правой
* Механизмы Join
  * **Broadcast Hash Join**
    * Как работает:
        * Наименьшая из таблица транслируется всем исполнителям
        * Для каждого транслированного набора строится хэш таблица
        * Далее они начинают соединяться без перетасовки
    * Плюсы
        * Отсутствие операций `sort` и `merge` 
    * Минусы
        * Требуют много места в памяти
            * Из-за этого Spark старается избегать этого, если размер обеих таблиц превышает определенный порог
                * Регулируется `spark.sql.autoBroadcastJoinThreshold`, который по умолчанию 10МБ
        * Только **Equi Join**
        * Не применим к **Full Outer Join**
  * **Shuffle Hash Join**
    * Как работает:
      * Shuffle
      * **Hash Join**
        * По меньшей таблице строится хэш и большая соединяется с ней
    * Плюсы
        * Меньшее требование по памяти нежели у Broadcast Hash Join
            * Меньше хэш таблицы и не надо транслировать таблицу по всем узлам
    * Из минусов
        * Только **Equi Join**
    * Примечание
        * Этот вид Join используется по умолчанию, когда `spark.sql.autoBroadcastJoinThreshold` превышен
        * Используется на болем объеме данных, когда нет перекоса в ключах
  * **Sort Merge Join** (он же Shuffle Sort Merge Join)
    * Как работает:
        * Shuffle 
        * Sort
        * Merge
          * Merge по сортированным данным проходит быстрее, чем обычный, так как после нахождения группы подходящих ключей все остальные не рассматриваются
    * Плюсы
      * Меньшнее потребление оперативки чем у **Shuffle Hash Join**
      * Поддерживает **Non-Equi Join**
    * Минусы 
        * За плюс выше обычно приходится платить скоростью фазы Merge 
        * Ключи должны поддаваться сортировке
    * Примечание
        * Используется для больших данных, но не настолько больших чтобы использовать **Shuffle Hash Join**
        * Хорошо работает на уже отсортированных данных
        * Нормально работает с неравномерно распределенными ключами
  * **Cartesian Product Join** (декартово соединение)
    * Используется исключительно для **Cross Join** либо для **Inner Join** без условия
    * Поддержка **Non-Equi Join**
    * Очень дорогая операция
  * **Broadcast Nested Loop Join**
    * Как работает:
      * Входной набор транслируется на всех
      * Проход вложенным циклом по всем ключам
    * Плюсы
      * Поддержка **Non-Equi Join**
    * Минусы
        * Много времени
        * Много памяти
* Хинты (подсказки) - дополнительная параметризация джоинов, которую может задать пользователь, чтобы оптимизировать расчеты
    * _Partitioning hints_ (подсказки по разделению) - эти хинты дают пользователям возможность настроить производительность и контролировать количество выходных файлов в Spark SQL
        * **COALESCE** пригодится для уменьшения количества разделов до указанного
        * **REPARTITION** можно использовать для перераспределения на указанное количество разделов с использованием указанных выражений партиционирования
        * **REPARTITION_BY_RANGE** подойдут для перераспределения на указанное количество разделов с использованием указанных выражений разделения
        * **REBALANCE** можно использовать для перебалансировки выходных разделов результатов запроса, чтобы каждый раздел был не слишком большим и не очень маленьким
    * _Join hints_(подсказки по соединению) - позволяют пользователям предлагать движку Spark SQL оптимальную стратегию соединения (но не гарантируют, что будет именно так)
        * **BROADCAST** — Broadcast Hash join. Сторона соединения с подсказкой будет транслироваться независимо от значения параметра _autoBroadcastJoinThreshold_
        * **MERGE** — Shuffle Sort Merge join. Псевдонимы для MERGE — SHUFFLE_MERGE и MERGEJOIN
        * **SHUFFLE_HASH** — Shuffle Hash join
        * **SHUFFLE_REPLICATE_NL**  — Cartesian Product join (соединение вложенных циклов с перемешиванием и репликацией)
``` Scala
val resDf = df1.hint("broadcast").join(df2, "columnName") // Join hints
resDf.coalesce(numPartitions)                             // Partitioning hints
```
### Ect
[Go Back](#оглавление)
#### Отличия Spark и PySpark
* Основным отличием является обработка UDF, в том плане, что Python работает кратно медленнее, чем Scala
* Некоторые методы работают иначе, у некоторых разные параметры, но это надо точечно сравнить, общей инфы нет
#### MapReduce как аналог Spark 
* Минусы MapReduce
  * Модель MapReduce выполняет вычисления за 2 этапа. В первую очередь он разделяет данные на части, передавая их на кластерные узлы для обработки. Потом каждый узел производит обработку данных с отправкой результата на главный узел, где и сформировывается итоговый результат распределенных вычислений
  * MapReduce регулярно обращается к диску, ведь именно там он сохраняет промежуточные и финальные итоги вычислений
  * Для написания хорошего решения на MapReduce понадобится довольно высокий уровень экспертности
* Плюсы Spark 
  * Spark выполняет обработку данных в памяти и, по сути, почти не обращается к диску
  * В Spark существует API для различных языков программирования, в результате чего писать код заметно проще
#### Файловые системы, поддерживаемые Spark
* HDFS
* Amazon S3
* Local File System
* Cassandra
* OpenStack Swift
* MapR File System
#### UDF 
* Пользовательские функции, которые позволяют расширять DataFrame API
* Методы UDF в Spark
    * asNonNullable() – обновляет UDF до значения, не допускающего NULL
    * asNondeterministic() – обновляет UDF до недетерминированного значения
    * withName(name: String) – обновляет UDF с заданным именем
##### Scala udf
* Пример того как на Scala написать и использовать Udf для Spark
``` Scala
import org.apache.spark.sql.functions.{col, udf}
import org.apache.spark.sql.{SparkSession => spark}

val random = udf(() => Math.random())
spark.udf.register("random", random.asNondeterministic())
spark.sql("Select random()")

val squared = udf((s: Long) => s * s)
dataFrame.select(squared(col("id")) as "id_squared"))
```
##### Python udf
* Пример того как на Python написать и использовать Udf для PySpark
``` Python
from pyspark.sql.functions import col, udf
from pyspark.sql.types import StringType

def upperCase(str):
    return str.upper()

upperCaseUDF = udf(lambda s: upperCase(s), StringType())
df.withColumn("Cureated Name", upperCaseUDF(col("Name")))

spark.udf.register("upperCase", upperCase, StringType())
spark.sql("select some, upperCase(Name) as UpName from NAME_TABLE") 
```
##### Pandas udf
* Пример того как на Python написать и использовать Pandas Udf для PySpark
``` Python
from pyspark.sql.functions import pandas_udf
from pyspark.sql.types import StringType
import pandas as pd

# Для Spark 3.0+
@pandas_udf(StringType())
def to_upper(s: pd.Series) -> pd.Series:
    return s.str.upper()

df.select("Seqno", "Name", to_upper("Name"))
```
##### Scala udf в PySpark
* Пример того как на Python написать и использовать Scala Udf для PySpark
``` Python
from my_udf.functions import square
from pyspark.sql import SparkSession, SQLContext

def square(s):
  return s * s

spark.udf.register("squareWithPython", square)
spark.sql("select id, squareWithPython(id) as id_square_sql from test")
sqlContext = SQLContext(spark.sparkContext)
spark._jvm.com.databricks.solutions.udf.Functions.registerFunc(sqlContext._jsqlContext, "cube")
spark.sql("select id, cube(id) as id_cube_sql_scala from test")

```
#### Кэширование
* Существует основная функция кэширования `persist()` или более простой её аналог `cache()`
    * У `cache()` уровень хранения задан по умолчанию: `MEMORY_ONLY` для RDD, `MEMORY_AND_DISK` для DataFrame и DataSet
* Уровни хранения
    * `MEMORY_ONLY` / `MEMORY_ONLY_SER` (`MEMORY_ONLY_2` / `MEMORY_ONLY_SER_2`) - кладет данные в общую кучу, если места не хватит, то при повторном обращении к данным будет проведен дополнительный перерасчет
    * `MEMORY_AND_DISK` / `MEMORY_AND_DISK_SER` (`MEMORY_AND_DISK_2` / `MEMORY_AND_DISK_SER_2`) - кладет данные в общую кучу, если места не хватит, то складывает их на диск, и забирает оттуда при необходимости
    * `DISK_ONLY` (`DISK_ONLY_2`, `DISK_ONLY_3`) - сохраняет данные на диск
    * `OFF_HEAP` - вне кучи, сегмент за пределами JVM. Надо дополнительно включать в Spark
    * `_2` / `_3` - означает что Spark будет реплицировать данные на 2х или 3х нодах (узлах кластера)
    * `_SER` - означает Java сериализацию данных
---
## Оркестраторы
[Go Back](#оглавление)
* Задачи, решаемые оркестратором:
  * Планирование задач — основная функция, позволяющая избавиться от ручного запуска рутинных задач по расчёту витрин, загрузке данных, резервному копированию
  * Управление зависимостями. Часто задачу нужно запустить не только в определённый промежуток времени, но и с учётом статуса других задач
  * Репроцессинг. Если известно, что какая-то задача требует перезапуска (например, были загружены неполные данные на предыдущем этапе), то перезапуска требуют и задачи, зависящие от неё. Кроме того, перезапуск может быть необходим за несколько временных периодов
  * Мониторинг
### Airflow
* В основе концепции Airflow лежит DAG — направленный ациклический граф. Он описывает процессы обработки данных и позволяет объединять задачи, определяя правила их совместной работы
  * https://airflow.apache.org/
* Airflow состоит из нескольких компонентов, но главные из них — scheduler и webserver. Без них ничего не запустится
  * Scheduler (планировщик) отслеживает все задачи и DAGs, а затем запускает экземпляры задач после установки их зависимостей. В фоновом режиме планировщик запускает подпроцесс, который отслеживает и синхронизирует все DAGs в указанном каталоге. По умолчанию он раз в минуту собирает результаты синтаксического анализа DAGs и проверяет, необходимо ли запустить какие-либо активные задачи
    * https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/scheduler.html
  * WebServer (веб-сервер) отвечает за отображение веб-интерфейса и аутентификацию пользователей, а также решает, к какой группе должен относиться тот или иной пользователь в соответствии с конфигурационным файлом
    * https://airflow.apache.org/docs/apache-airflow/stable/security/webserver.html
* Преимущетсва
    * Небольшой, но полноценный инструментарий создания процессов обработки данных и управления ими – 3 вида операторов (сенсоры, обработчики и трансферы), расписание запусков для каждой цепочки задач, логгирование сбоев
    * графический веб-интерфейсдля создания конвейеров данных (data pipeline), который обеспечивает относительно низкий порог входа в технологию
    * Расширяемый REST API, который относительно легко интегрировать Airflow в существующий ИТ-ландшафт корпоративной инфраструктуры и гибко настраивать конвейеры данных, например, передавать POST-параметры в DAG
    * Программный код на Python
    * Интеграция со множеством источников и сервисов
    * Наличие собственного репозитория метаданных на базе библиотеки SqlAlchemy, где хранятся состояния задач, DAG’ов, глобальные переменные и пр
    * Масштабируемость за счет модульной архитектуры и очереди сообщений для неограниченного числа DAG’ов
* Недостатки
    * Наличие неявных зависимостей при установке, например, дополнительные пакеты типа greenlet, gevent, cryptography и пр. усложняют быстрое конфигурирование этого фреймворка
    * Большие накладные расходы (временная задержка 5-10 секунд) на постановку DAG’ов в очередь и приоритизацию задач при запуске
    * Необходимость наличия свободного слота в пуле задач и рабочего экземпляра планировщика
        * Например, можно выделять сенсоры (операторы для извлечения данных, операция Extract в ETL-процессе) в отдельный пул, чтобы таким образом контролировать количество и приоретизировать их, сокращая общую временную задержку (latency)
    * Пост-фактум оповещения о сбоях в конвейере данных, в частности, в интерфейсе Airflow логи появятся только после того, как задание, к примеру, Spark-job, отработано. Поэтому следить в режиме онлайн, как выполняется data pipeline, приходится из других мест, например, веб-интерфейса YARN
    * Разделение по операторам — каждый оператор Airflow исполняется в своем python-интерпретаторе
        * Файл, который создается для определения DAG – это не просто скрипт, который обрабатывает какие-то данные, а объект. В процессе выполнения задачи DAG не могут пересекаться, так как они выполняются на разных объектах и в разное время. При этом на практике иногда возникает потребности, чтобы несколько операторов Airflow могли выполняться в одном Spark-контексте над общим пространством dataframe’ов.
* Пример запуска https://ajar-raclette-d2c.notion.site/Airflow-ad4206450dd74a3e85d755a3de014725
### Oozie
* Это серверная система планирования выполнения рабочих процессов и повторяющихся задач в экосистеме Hadoop
* Рабочие процессы в Oozie представлены в виде DAG-цепочки
* Oozie поддерживает запуск задач Hadoop MapReduce, Apache Hive, Pig, Sqoop, Spark, операций HDFS, UNIX Shell, SSH и электронной почты, а также может быть расширен для поддержки дополнительных действий
* Задачи могут быть настроены для регулярного выполнения или разово при возникновении некоторого события
* Oozie целесообразно использовать, когда поток обработки включает несколько шагов, каждый из которых зависит от предыдущего. Например, запуск скрипта для проверки наличия ожидаемых входных данных перед их фактической обработкой
* Oozie использует контейнер oozie-launcher для каждой запускаемой задачи. Когда несколько заданий запускаются одновременно, есть вероятность, что в кластере запущены все oozie-launcher, но ни одно из заданий Spark. Пока все эти контейнеры oozie-launcher ожидают завершения своих соответствующих заданий Spark, фактически ни одно из них не запущено. Это означает, что кластер заполнен oozie-launcher’ами, но практическая работа не выполняется
* Поскольку для каждого типа задач (Pig, Hive, Sqoop, Spark и пр.) в Oozie требуются собственные jar-файлы и версии библиотек, это может привести к тому, что разные несовместимые версии jar-библиотеки могут вызывать сбои в заданиях
### Luigi
* Фреймворк на языке Python для построения сложных последовательностей по выполнению зависимых задач
* Довольно большая часть фреймворка направлена на преобразования данных из различных источников (MySql, Mongo, redis, hdfs) и с помощью различных инструментов (от запуска процесса до выполнения задач разных типов на кластере Hadoop)
* Самое главное преимущество фреймворка — возможность выстраивать последовательности зависимых задач
* Фреймворк разрешает зависимости, отслеживает граф выполнения, управляет запуском задач, обрабатывает ошибки с возможностью перезапуска нужных задач, распределяет ресурсы рабочих процессов с возможностью параллельной работы независимых частей графа задач
* Отсутствие механизма запуска задач по расписанию, что вызывает необходимость использования crontab
* Трудности масштабирования из-за слишком тесной связи DAG-задач с cron-заданиями, что ограничивает количество рабочих процессов
* Luigi не может автоматически распределять задачи между worker’ами на разных узлах
* Перезапуск конвейеров данных невозможен
* Неудобство GUI
* Отсутствие предварительной проверки выполнения задачи
### Livy
* Это REST-интерфейс для кластера Spark, который позволяет запускать и отслеживать отдельные задания Spark, напрямую используя фрагменты кода Spark или предварительно скомпилированные jar-файлы
* Можно использовать для запуска «асинхронных» заданий Spark, которые могут быть запущены в любое время, и не требуют обязательного отклика
* Можно применять для оркестрации рабочих процессов с опросом API для ответа о состоянии задания
* Миграция между версиями Spark выполняется автоматически: если задание выполняется в более новой версии Spark, Livy просто запустит его
* Не требуется разрабатывать дополнительные workflow-файлы и свойства для запуска заданий, а используется только простой HTTP-интерфейс
* Не является полноценной системой управления потоками работ, а используется исключительно для запуска и отслеживания отдельных заданий Spark
---
## Прочие полезности
[Go Back](#оглавление)
### Hadoop
* Hadoop – это свободно распространяемый набор утилит, библиотек и фреймворк для разработки и выполнения распределённых программ, работающих на кластерах из сотен и тысяч узлов. Эта основополагающая технология хранения и обработки больших данных (Big Data) является проектом верхнего уровня фонда Apache Software Foundation.
  * Изначально писался на Java под MapReduce
* Список полезных команд (HDFS) https://docs.arenadata.io/ru/ADH/current/references/hdfs-cheatsheet.html
* Модули
  * **HDFS** - распределенная файловая система хадупа (Hadoop Distributed File System)
    * Являет собой технологию хранения данных на разных серверах (узлах/нодах)
    * Обеспечивает надежность за счет репликации (дублирования данных)
    * Архитектура
      * NameNode - основонй отдельный сервер для управления пространством имен файловой системы, хранящий дерево файлов, а также мета-данные файлов и каталогов
      * Secondary NameNode - дублирование NameNode для отказоустойчивости и быстрого восстановления. Находится на кластере в единственном экземпляре
      * DataNode - серверА с программным кодом, отвечающим за файловые операции и работу с блоками данных
      * Client - пользователь/приложение взаимодействующие через API с HDFS
    * Особенности
      * Число репликаций по умолчанию 3
      * Размер файла по умолчанию 64 Мб
      * Спокойно хранит файлы размером больше 10 Gb
      * Не требует дорогого серверного оборудования
      * Репликация асинхронная и осуществляется на уровне кластера, а не на уровне узлов
      * Хорошая оптимизация для потоковых операций
      * Файловые операции выполняются асинхронно для разных файлов
      * Принцип WORM (Write-once and read-many, один раз записать – много раз прочитать) полностью освобождает систему от блокировок типа «запись-чтение». Запись в файл в одно время доступен только одному процессу, что исключает конфликты множественной записи
      * Хорошо настроенное сжатие данных -> экономия дискового пространства
      * Самодиагностика
      * Метаданные сервера имен хранятся в оперативной памяти -> быстрый доступ
    * Недостатки
      * При поломке NameNode кластер не работает, до восстановления
      * Нет репликации для Secondary NameNode
      * Нет возможности редактирования сохраненных файлов
      * Нет проверки целостности данных
      * Нет поддержки реляционных моделей данных
  * **YARN** - ресурсный менеджер Yet Another Resource Negotiator
    * Архитектура
      * ResourceManager (RM) — менеджер ресурсов, которых отвечает за распределение ресурсов, необходимых для работы распределенных приложений, и наблюдение за узлами кластера, где эти приложения выполняются. ResourceManager включает планировщик ресурсов (Scheduler) и диспетчер приложений (ApplicationsManager, AsM)
      * ApplicationMaster (AM) – мастер приложения, ответственный за планирование его жизненного цикла, координацию и отслеживание статуса выполнения, включая динамическое масштабирование потребления ресурсов, управление потоком выполнения, обработку ошибок и искажений вычислений, выполнение локальных оптимизаций
      * NodeManager (NM) – менеджер узла – агент, запущенный на узле кластера, который отвечает за отслеживание используемых вычислительных ресурсов (CPU, RAM и пр.), управление логами и отправку отчетов об использовании ресурсов планировщику. NodeManager управляет абстрактными контейнерами – ресурсами узла, доступными для конкретного приложения
      * Контейнер (Container) — набор физических ресурсов (ЦП, память, диск, сеть) в одном вычислительном узле кластера
    * Принципы работы YARN 
      * Клиентское приложение отправляет запрос в кластер
      * Менеджер ресурсов выделяет необходимые ресурсы для контейнера и запускает ApplicationMaster для обслуживания этого приложения
      * ApplicationMaster отправляет запрос менеджеру узла NodeManager, включая контекст запуска контейнера Container Launch Context (CLC)
      * ApplicationMaster выделяет контейнеры для приложения в каждом узле и контролирует их работу до завершения работы приложения
      * Для запуска контейнера менеджер узла копирует в локальное хранилище все необходимые зависимости (данные, исполняемые файлы, архивы)
      * По завершении задачи мастер приложения отменяет выделенный контейнер в диспетчере ресурсов, завершая жизненный цикл распределенного задания
      * Клиент может отслеживать состояние распределенного приложения, обращаясь к менеджеру ресурсов или сразу к мастеру приложения
  * **Hadoop Common** - набор инфраструктурных программных библиотек и утилит, которые используются в других решениях и родственных проектах, в частности, для управления распределенными файлами и создания необходимой инфраструктуры
  * **Hadoop MapReduce** - платформа программирования и выполнения распределённых MapReduce-вычислений, с использованием большого количества компьютеров (узлов, nodes), образующих кластер
### Kafka
[Go Back](#оглавление)
* Это распределенная система обмена сообщениями между серверными приложениями в режиме реального времени
* Основы кластера Kafka — это **продюсер**, **брокер** и **консумер**. Продюсер пишет сообщения в **лог** брокера, а консумер его читает
* **Лог** — это упорядоченный поток событий во времени. Событие происходит, попадает в конец лога и остаётся там неизменным
* **Продюсеры** — это приложения для записи событий в кластер Kafka
    * Один продюсер может писать в один или несколько топиков
    * Продюсеры самостоятельно партицируют данные в топиках и сами определяют алгоритм партицирования: он может быть, как банальный round-robin и hash-based, так и кастомный
    * Продюсер сам выбирает размер **батча** и число ретраев при отправке сообщений. Протокол Kafka предоставляет гарантии доставки всех трёх семантик: at-most once, at-least once и exactly-once
* **Брокеры** — это "локация/сервер" где хранятся события
    * Кластер Kafka состоит из брокеров
    * Все брокеры соединены друг с другом сетью и действуют сообща, образуя единый кластер
    * Когда мы говорим, что продюсеры пишут события в Kafka-кластер, то подразумеваем, что они работают с брокерами в нём
* **Топик** — это логическое разделение категорий сообщений на группы (формально один топик - один лог)
    * Топики в Kafka разделены на партиции
    * Увеличение партиций увеличивает параллелизм чтения и записи
    * Партиции находятся на одном или нескольких брокерах, что позволяет кластеру масштабироваться
    * Партиции хранятся на локальных дисках брокеров и представлены набором лог-файлов — **сегментов**
    * Каждое сообщение в таком логе определяется порядковым номером — **оффсетом**. Этот номер монотонно увеличивается при записи для каждой партиции
    * Лог-файлы на диске устаревают по времени или размеру. Настроить это можно глобально или индивидуально в каждом топике
    * Для отказоустойчивости, партиции могут реплицироваться. Число реплик или **фактор репликации** настраивается как глобально по умолчанию, так и отдельно в каждом топике
    * Реплики партициий могут быть **лидерами** или **фолловерами**. Традиционно консумеры и продюсеры работают с лидерами, а фолловеры только догоняют лидера
* **Консумеры** это приложения которые могут читать события из брокеров
    * Один консумер может читать один или несколько топиков
* Чтение лога
    * Сегмент тоже удобно представить, как обычный лог-файл: каждая следующая запись добавляется в конец файла и не меняет предыдущих записей. Фактически это очередь FIFO (First-In-First-Out) и Kafka реализует именно эту модель
    * Семантически и физически сообщения внутри сегмента не могут быть удалены, они иммутабельны. Всё, что мы можем — указать, как долго Kafka-брокер будет хранить события через настройку политики устаревания данных или **Retention Policy**
    * Числа внутри сегмента — это реальные сообщения системы, у которых есть порядковые номера или оффсеты, что монотонно увеличиваются со временем. У каждой партиции свой собственный счётчик, и он никак не пересекается с другими партициями
    * Начальная позиция первого сообщения в логе называется **log-start offset**. Позиция сообщения, записанного последним — **log-end offset**. Позиция консумера сейчас — **current offset**
    * Расстояние между конечным оффсетом и текущим оффсетом консумера называют **лагом** — это первое, за чем стоит следить в своих приложениях
* Событие — это пара ключ-значение
    * Ключ партицирования может быть любой: числовой, строковый, объект или вовсе пустой
    * Значение тоже может быть любым — числом, строкой или объектом в своей предметной области, который вы можете как-то сериализовать (JSON, Protobuf, …) и хранить
* Кластер Kafka позволяет изолировать консумеры и продюсеры друг от друга. Продюсер ничего не знает о консумерах при записи данных в брокер, а консумер ничего не знает о продюсере данных
* Zookeeper — это выделенный кластер серверов для образования кворума-согласия и поддержки внутренних процессов Kafka
    * Помогает обнаружить сбои контроллера, выбрать новый и сохранить работоспособность кластера Kafka
### Flink
[Go Back](#оглавление)
* Apache Flink – это фреймворк и движок для statefull вычислений над неограниченными и ограниченными потоками данных
* Программы можно писать, как в параллельном режиме, так и пайплайнам
* Позволяет реализовать последовательность заданий (batch) и поток заданий (stream)
* Достаточно хорошо оптимизирован и обладает высокой пропускной способностью и низкими задержками
* Поддерживает приложения на Java, Scala, Python и SQL
* Задачи в Flink устойчивы к отказам и используют строго одну семантику
* Flink не обладает собственной системой хранения данных, но использует коннекторы для различных источников данных, таких как Apache Kafka, HDFS, Apache Cassandra, ElasticSearch, Amazon Kinesis и другие
* Для разработки программ есть три основных API - DataStream API, Table API и Python API
#### Flink vs Spark
* Сходства
    * Поддержка лямбда-архитектуры
    * Наличие встроенных средств для выполнения графовых операций, машинного обучения и SQL-аналитики
    * Гарантия строго однократной доставки сообщений
    * Надежность, масштабируемость и отказоустойчивость
* Различия
    * Время задержки (latency)
        * Flink обеспечивает обработку данных в режиме реального времени с задержкой порядка 1 миллисекунды
        * Для Spark latency может составлять несколько секунд
    * Режим работы с данными
        * Несмотря на то, что Spark является средством потоковой обработки Big Data, он реализует микропакетный подход (micro-batch)
        * Flink обеспечивает полноценную работу как в поточном, так и в пакетном режимах с помощью разных API – для потоков DataStream API и DataSet API для пакетов
    * Зрелость, поддержка сообщества и реализация в коммерческих продуктах
        * В настоящее время Apache Spark более распространен среди Big Data решений и сообщества специалистов.
        * Для Flink пока характерны некоторые проблемы на уровне реализации, например, высокое потребление CPU, чувствительность к сетевым проблемам, но доля этих недостатков снижается с каждым новым релизом
### Хранение данных
[Go Back](#оглавление)
#### Data Warehouse
* Это ___хранилище___, в которое из разных систем хранения собираются исторические данные компании
    * Это некая библиотека, в которой упорядочена и каталогизирован весь объем информации
    * Она может быть в основе, например, платформы обработки данных
* **Особенности**
    * Аналитику не нужно запрашивать доступы к базам данных разных отделов. Все хранится в одном месте, при этом в DWH могут храниться агрегированные данные за десятки лет
    * Данные в хранилище добавляются, удаляются, очищаются, выгружаются. К этому хранилищу выполняются запросы, также с ним производятся другие манипуляции
    * При использовании систем бизнес-аналитики (BI) совместно с DWH у пользователей появляется возможность искать закономерности и взаимосвязи в данных, аналитически обрабатывать и визуализировать информацию. Аналитик изучает данные из хранилища, формирует отчет, подкрепляя статистической информацией, визуализирует
* Хранит преобразованные и структурированные данные из разных источников: CRM и транзакционных систем, сервисов для создания рекламы и т. д.
    * Бизнес-аналитики, маркетологи, дата-инженеры могут получить к ним доступ с помощью инструментов BI, SQL-клиентов и других аналитических приложений
* Традиционный вид архитектуры
    * **Нижний уровень** — база данных (или даже несколько), которые объединяют в себе данные из различных источников информации — например, из транзакционных СУБД или SaaS-сервисов.
    * **Средний уровень** — сервисы и приложения, которые преобразуют данные в специальную структуру для анализа и сложных запросов (уровень моделирования, либо семантической слой).
        * Это может быть сервер OLAP, например, который работает в качестве расширенной системы управления реляционными базами данных. И отображает операции над многомерными данными в стандартных реляционных операциях. 
    * **Верхний уровень** — инструменты для создания отчетов, визуализации и последующего анализа данных. Его также называют уровнем клиента. 
* Модели (методологии) проектирования DWH
    * **Data Vault**
        * Data Vault — гибридный подход, объединивший достоинства знакомой многим схемы «звезды» и 3-ей нормальной формы
        * Data Vault состоит из трех основных компонентов — **Хаб** (Hub), **Ссылка** (Link) и **Сателлит** (Satellite)
        * **Хаб** — основное представление сущности (Клиент, Продукт, Заказ) с позиции бизнеса. Таблица-Хаб содержит одно или несколько полей, отражающих сущность в понятиях бизнеса
        * **Таблицы-Ссылки** связывают несколько хабов связью многие-ко-многим. Они содержат те же метаданные, что и Хаб.
            * Ссылка может быть связана с другой Ссылкой, но такой подход создает проблемы при загрузке, так что лучше выделить одну из Ссылок в отдельный Хаб
        * Все описательные атрибуты Хаба или Ссылки (контекст) помещаются в **таблицы-Сателлиты**. Помимо контекста Сателлит содержит стандартный набор метаданных (load timestamp и record source) и один и только один ключ «родителя».
            * В Сателлитах можно без проблем хранить историю изменения контекста, каждый раз добавляя новую запись при обновлении контекста в системе-источнике
    * **Слоеный пирог**
        * **Стейджинг** (Primary Data Layer) — уровень, на котором подгружаются данные из внешних источников. Например, из таблиц, ERP-системы или биллинговой системы
        * **Ядро хранилища** (Core Data Layer) — центральный уровень, который подгоняет данные к единым структурам и ключам. На этом слое обеспечивается целостность и качество данных
        * **Аналитические витрины** (Data Mart Layer) — слой, который преобразует данные к структурам, удобным для анализа и использования в BI-дашбордах и других аналитических системах
        * **Сервисный слой** (Service Layer) — уровень, на котором обеспечивается управление предыдущими слоями, мониторинг и диагностика ошибок
    * **Модель Инмона**
        * По модели Инмона (Inmon) данные из источников должны поступать в хранилище после процесса ETL (Extract, Transformation, Load)
        * Хранилища данных, это «предметно-ориентированный, энергонезависимый, интегрированный, изменяющийся во времени набор данных для поддержки управленческих решений»
        * Модель создает тщательную логическую модель для каждого основного объекта
        * Эта логическая модель может включать десять различных сущностей продукта, включая все детали, такие как бизнес-факторы, аспекты, отношения, зависимости и связи
        * Использует нормализованную форму для построения структуры объекта, максимально избегая избыточности данных. Это приводит к четкому определению бизнес-требований и предотвращению любых нарушений при обновлении данных
        * Далее строится физическая модель, которая соответствует нормализованной структуре
    * **Модель Кимбалла**
        * По модели Кимбалла (Kimball) после процесса ETL данные загружаются в витрины данных, а объединение витрин создает концептуальное (а не фактическое) хранилище данных
        * Витрины данных сначала формируются на основе бизнес-требований
        * Затем оцениваются первичные источники данных, и Инструмент ETL используется для получения данных из нескольких источников и загрузки их в промежуточную область сервера реляционной базы данных
        * После загрузки данных в промежуточную область хранилища данных следующий этап включает загрузку данных в многомерную модель хранилища данных, которая по своей природе денормализована
        * Эта модель разделяет данные на таблицу фактов, которая представляет собой числовые данные транзакций, или таблицу измерений, которая является справочной информацией, подтверждающей факты
    * **Anchor Modeling**
        * Гибкий метод моделирования, подходящий для работы с постоянно растущими объемами данных, которые меняются по структуре или содержанию.
        * Якорная модель позволяет воспользоваться преимуществами высокой степени нормализации, при этом оставаясь интуитивно понятной
        * **Якорь** – представляет собой сущность или событие, содержит суррогатные ключи, ссылку на источник и время добавления записи
        * **Атрибут** – используется для моделирования свойств и характеристик якорей, содержит суррогатный ключ якоря, значение атрибута, ссылку на источник записи и время добавления записи
        * **Связь** – моделирует отношения между якорями
        * **Узел** – используется для моделирования общих свойств (состояния)
    * **Снежинка** и **звезда**
        * **Звезда** имеет централизованное хранилище данных, которое хранится в таблице фактов
        * Схема разбивает таблицу фактов на ряд денормализованных таблиц измерений
        * Таблица фактов содержит агрегированные данные, которые будут использоваться для составления отчетов, а таблица измерений описывает хранимые данные
        * **Снежинка** отличается тем, что использует нормализованные данные
        * Нормализация означает эффективную организацию данных так, чтобы все зависимости данных были определены, и каждая таблица содержала минимум избыточности
            * Таким образом, отдельные таблицы измерений разветвляются на отдельные таблицы измерений
        * **Снежинка** использует меньше дискового пространства и лучше сохраняет целостность данных. Основным недостатком является сложность запросов, необходимых для доступа к данным — каждый запрос должен пройти несколько соединений таблиц, чтобы получить соответствующие данные
#### DataLake
[Go Back](#оглавление)
* Централизованное хранилище, в котором хранятся структурированные, полуструктурированные и неструктурированные исходные данные
* Озёра данных могут получать информацию в режиме реального времени
    * В Data lake можно загружать данные без информационных потерь
* Подход к архитектуре, в котором создается промежуточное хранилище, куда сваливаются сырые данные из всех источников
    * Внутри этого хранилища они обрабатываются, агрегируются, декомпозируются, композируются, нормализуются и улетают в базу данных
    * Данные собираются со всех источников, потом складываются в Data Lake
    * Далее они преобразуются и отправляются в основное хранилище, где все это хранится в красивом виде, без лишней информации
    * В то же время, учитывая, что сохраняется копия данных, всегда можно к ним вернуться
    * Если трансформация получилась плохой, есть данные из источника, к которым можно сходить, и еще раз пересобрать
* Основные задачи, которые решает Data Lake
    * Хранение исторических данных из первичных источников, и последующий анализ, переработка данных, что используется в Big Data и ML
* Data Lake не имеют схемы и более гибки для хранения реляционных данных из бизнес-приложений, а также нереляционных журналов с серверов и, например, социальных сетей
#### Data Lakehouse
[Go Back](#оглавление)
* Гибридное решение для хранения данных. Оно сочетает лучшие свойства хранилищ и озёр данных. Data lakehouse может хранить все виды данных и позволяет управлять метаданными
* Data Lakehouse стоит меньше, чем data warehouse, и проще масштабируется
* Delta Lake - частое исполнение концепции Data Lakehouse
* Delta Lake поддерживает ACID-транзакции и масштабируемую обработку метаданных, объединяя потоковые и пакетные операции с большими данными. Примечательно, что Delta Lake работает на базе существующего озера данных (на Apache Hadoop HDFS, Amazon S3 или Azure Data Lake Storage) и полностью совместимо со всеми API Apache Spark
* Преимущества Delta Lake (Data Lakehouse)
    * Поддержка ACID-транзакций – обычно озера данных имеют несколько конвейеров их обработки (data pipelines), которые одновременно читают и записывают информацию. Поэтому инженерам данных приходится обеспечивать целостность данных из-за отсутствия транзакций. Delta Lake переносит ACID-транзакции в озера данных, обеспечивая сериализуемость и самый высокий уровень изоляции
    * Масштабируемая обработка метаданных – поскольку в Big Data даже данные о данных (метаданные) могут быть очень большими, Delta Lake обрабатывает их, используя распределенную вычислительную мощность Apache Это позволяет легко обрабатывать таблицы петабайтного масштаба с миллиардами разделов и файлов
    * Управление версиями данных – Delta Lake предоставляет моментальные снимки данных, позволяя разработчикам получать доступ и возвращаться к более ранним версиям для аудита, отката или воспроизведения экспериментов, что мы рассматриваем здесь
         * Аудит истории изменений с помощью журнала транзакций, куда записываются сведения о каждом изменении данных
обновления и удаления наборов данных (Update/Delete) с помощью API Scala/Java, что упрощает сбор измененных данных
    * Открытый формат – все данные в Delta Lake хранятся в столбцовом формате Apache Parquet, что позволяет использовать эффективно сжимать и кодировать данные.
    * Унифицированный пакетный и потоковый источник и приемник в одном – таблица в Delta Lake одновременно является пакетной таблицей, а также источником и приемником потоковой передачи
         * Потоковая загрузка данных, пакетная обработка исторической информации и интерактивные запросы работают сразу после установки
    * Поддержка схем данных – что позволяет указать структуру информации и обеспечить ее соблюдение, гарантируя правильность типов данных и наличие необходимых столбцов. Это предупреждает повреждение информации из-за некорректных данных
         * Эволюция схемы – позволяет вносить изменения в схему таблицы, которые могут применяться автоматически, без необходимости использования громоздкого DDL (Data Definition Language — семейство языков для описания структуры СУБД).
    * Полная совместимость с Apache Spark API, благодаря чему разработчики могут запускать на Delta Lake существующие конвейеры обработки данных с минимальными изменениями.
### Форматы данных в BigData
[Go Back](#оглавление)
* **Линейные** (строковые)
  * Строки данных хранятся вместе, образуя непрерывное хранилище
  * Пониженная скорость чтения и выполнения избирательных запросов
  * Большой расход дискового пространства
  * Лучше подходят для потоковой записи
  * Виды
    * `AVRO`
      * Может использовать компактную бинарную кодировку или человекочитаемый формат JSON  
      * Обеспечивает высокую скорость записи
      * Подходит для Apache Kafka, Flume, DataLake
    * `Sequence`
      * Двоичный формат для хранения данных в виде сериализованных пар ключ/значение 
      * Обеспечивает хороший параллелизм при MapReduce  
* **Колоночные** (столбцовые)
  * Столбцы хранятся вместе, но могут обрабатываться отдельно друг от друга
  * Быстрое чтение данных в избирательных запросах
  * Занимает много оперативной памяти
  * Не подходит для потоковой записи, т.к. файл не может быть восстановлен из-за отсутствия точек синхронизации
  * Занимает меньше дискового пространства
  * Виды
    * `Parquet`
      * Бинарный, колоночно-ориентированный формат
      * Подходит для Kafka, Spark и Hadoop
      * Поддерживает boolean, int32, int64, int96, float, double, byte_array
      * *Достоинства*
        * Экономия места
        * Высокая скорость
        * Возможность реализации собственных схем данных
        * Многие ЯП поддерживают
        * Возможность хранения данных не только в HDFS
        * Простота и удобство работы с файлами
        * Поддержка Apache Spark «по умолчанию»
      * *Недостатки*
        * Строгая типизация данных
        * Отсутствие встроенной (нативной) поддержки в других фреймворках, кроме Apache Spark
        * Отсутствие возможности отслеживать изменение данных и эволюцию схемы
        * Сложность частичной потоковой передачи данных
        * Сильная привязка к метаданным
        * Паркет не является человекочитаемым форматом
    * `RCFile`
      * Гибридный многоколонный формат записей, адаптированный для хранения реляционных таблиц на кластерах и предназначенный для систем, использующих MapReduce
    * `ORC`
      * *Достоинства*
        * Один файл на выходе любой задачи — это снижает нагрузку на узел имен (NameNode)
        * Поддерживается тип данных Hive, сложные и десятичные типы данных (list, map, struct, union)
        * Возможно одновременное считывание того же файла различными процессами RecordReader
        * Можно разделить файлы без сканирования этих файлов на предмет наличия маркеров
        * Индексация блоков для каждого столбца
        * Генерация наиболее эффективного графа при оптимизации SQL-запросов
### ETL инструменты
* Инструменты ETL используют, когда нужно быстро перенести много разнородных данных. Такие решения автоматизируют процесс и экономят ресурсы, потому что вам не придётся создавать собственные конвейеры данных
* Популярные ETL инструменты
    * Astera
    * Integrate
    * Fivetran
    * Talend
    * Hevo Data
    * Skyvia
    * SAS Data Management
    * Pentaho
    * Stitch
    * Blendo
    * Airbyte
    * Azure Data Factory
    * Oracle Data Integrator
    * Apache Airflow
    * Portable.io
---
# Источники
[Go Back](#оглавление)
* https://github.com/OBenner/data-engineering-interview-questions/blob/master/content/spark.md
* https://github.com/yakimka/python_interview_questions/blob/master/questions.md/#Python
* https://habr.com/ru/companies/kuper/articles/738634/
* https://habr.com/ru/articles/828984/
* https://spark-school.ru/blog/how-catalyst-works/
* https://bigdataschool.ru/blog/what-is-tungsten-for-spark-and-how-it-works.html
* https://bigdataschool.ru/blog/rdd-vs-dataframe-vs-dataset.html
* https://habr.com/ru/articles/253877/
* https://habr.com/ru/articles/258443/
* https://bigdataschool.ru/blog/broadcast-join-in-spark-sql-and-other-hints-example.html
* https://sparkbyexamples.com/spark/spark-rdd-vs-dataframe-vs-dataset/
* https://docs.aws.amazon.com/prescriptive-guidance/latest/spark-tuning-glue-emr/using-join-hints-in-spark-sql.html

